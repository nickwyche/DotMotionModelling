[
  {
    "objectID": "responsebias_circularmodel.html",
    "href": "responsebias_circularmodel.html",
    "title": "Response bias modelling",
    "section": "",
    "text": "After Friday’s meeting I had a think about how we might model response bias in the dot motion task. At the moment there are a few problems with our analytic approach:\nI went back to the drawing board and realised that we can simply model circular responses directly in the R package brms. This solves all of our problems:\nThese models are based off the von Mises distribution (circular equivalent of the normal distribution), which is defined by two parameters:\nI’ve fit two models to the data in brms, as follows:"
  },
  {
    "objectID": "responsebias_circularmodel.html#core-and-catch-trials-reanalysis",
    "href": "responsebias_circularmodel.html#core-and-catch-trials-reanalysis",
    "title": "Response bias modelling",
    "section": "Core and catch trials: reanalysis",
    "text": "Core and catch trials: reanalysis\nCore trials made up the majority (90%; 432/480 trials) of the task. They’re what we’ve discussed in meetings so far: participants saw an equal number of trials from each cell of our 2 x 2 design (2 reward levels: low, high; 2 coherence levels: 20%, 40%).\nCatch trials (those with coherence 60%) comprised 5% of the total number of trials in the experiment (24/480), with an equal number of catch trials in each block. There were an equal number of high-reward and low-reward catch trials. In theory, these trials are so easy that we shouldn’t be seeing any reward-related bias in responding.\nBecause catch trials basically add another level of coherence to the core 2 x 2 design, they can be directly added into a combined model. One thing to keep in mind is that estimation will often be less precise for 60% coherence trials as the model is informed by less data from this trial type.\n\nDoes the model fit?\nLet’s first visually inspect how the model is fitting relative to the observed data.\n\n\n\n\n\n\n\n\n\nOverall, model fit is looking fairly good. The key concern is that it’s slightly underestimating the heights of the distributions’ peaks, and for 40% coherence the ‘shoulders’ are a bit too wide. Both of these things suggest we should tweak priors for response precision, and/or fit a mixture model that accounts for guessing also. For now, this fit is near enough that I think we can take the following results as indicators of what we’d expect to see from a mixture modelling approach.\n\n\nIs a response bias present?\nFirst let’s see how the model has estimated mu (mean) in each reward x coherence condition.\n\n\n\nResponse bias by condition (degrees)\n\n\nReward\nCoherence\nMedian Bias\n95% Lower\n95% Upper\nP(&gt;0)\n\n\n\n\nhi\n0.2\n4.36\n2.16\n6.51\n1.00\n\n\nhi\n0.4\n2.06\n0.26\n3.83\n0.99\n\n\nhi\n0.6\n1.61\n-0.52\n3.62\n0.93\n\n\nlo\n0.2\n1.50\n-0.65\n3.64\n0.91\n\n\nlo\n0.4\n2.64\n0.91\n4.37\n1.00\n\n\nlo\n0.6\n3.03\n0.74\n5.29\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTakeaways:\n\nWhen coherence is low (20%), there is a credible response bias for high-value trials but not low-value ones\nWhen coherence is medium (40%), there is a credible (but small) response bias for both high and low-value trials.\nWhen coherence is high (60%), there is no longer a response bias for high-value trials. Conversely, the response bias for low-value trials persists.\n\n\n\nHow does bias change with coherence and reward?\nThe nice thing about this modelling approach is that we can see the posterior distribution for the size of changes to response bias across levels of coherence and reward (i.e., the range of plausible values for how much the response has actually shifted). In these graphs, the plotted point is the median difference and the whiskers are the 66% (thick) and 95% (thin) credible intervals.\n\nEffect of coherence\nFirstly, let’s take a look at how coherence affects response bias within each level of reward.\n\n\n\n\n\n\n\n\n\n\nChange in bias across coherence levels\n\n\nReward Level\nMedian Diff\nLower 95%\nUpper 95%\nP(Diff)\n\n\n\n\nhi coherence0.2 - hi coherence0.4\n2.30\n0.68\n3.89\n1.00\n\n\nhi coherence0.2 - hi coherence0.6\n2.76\n0.81\n4.70\n1.00\n\n\nhi coherence0.4 - hi coherence0.6\n0.46\n-1.06\n1.99\n0.73\n\n\nlo coherence0.2 - lo coherence0.4\n-1.14\n-2.81\n0.51\n0.91\n\n\nlo coherence0.2 - lo coherence0.6\n-1.53\n-3.67\n0.63\n0.92\n\n\nlo coherence0.4 - lo coherence0.6\n-0.40\n-2.15\n1.40\n0.67\n\n\n\n\n\n\n\nTakeaways:\n\nFor low-reward trials, the bias is stable: it doesn’t reliably change across any of the coherence comparisons\nFor high-reward trials, the exceptional condition is low coherence: bias in this condition is credibly higher than in either the medium or high coherence condition. Note that the bias doesn’t credibly differ across the two higher-coherence conditions.\n\n\n\nEffect of reward\n\n\n\n\n\n\n\n\n\n\nCore trials: change in bias across reward levels\n\n\nCoherence Level\nMedian Diff\nLower 95%\nUpper 95%\nP(Diff)\n\n\n\n\nhi coherence0.6 - lo coherence0.6\n-1.43\n-3.46\n0.61\n0.92\n\n\nhi coherence0.4 - lo coherence0.4\n-0.57\n-1.70\n0.53\n0.85\n\n\nhi coherence0.2 - lo coherence0.2\n2.86\n0.78\n4.95\n1.00\n\n\n\n\n\n\n\nTakeaways:\n\nWhen coherence is low, response bias is credibly larger for high-reward trials than low-reward ones\nWhen coherence is medium and high, there is no credible difference in response bias based on reward level, but the trend suggests the bias may be inverting (i.e., crossover interaction: as coherence increases, participants show more bias on low-value trials than high-value ones)\n\n\n\n\nDoes response precision change across trial types?\nNext, let’s see how the model has estimated kappa (response precision) in each reward x coherence condition.\n\n\n\n\n\n\n\n\n\n\nCore trials: response precision (kappa) by condition\n\n\n\n\n\n\n\n\n\n\n\n95% Credible Interval\n\n\n\nCoherence\nReward\nMedian Kappa\n95% Lower\n95% Upper\n\n\n\n\n0.2\nhi\n0.599\n0.483\n0.742\n\n\nlo\n0.597\n0.486\n0.737\n\n\n0.4\nhi\n1.491\n1.204\n1.853\n\n\nlo\n1.370\n1.115\n1.684\n\n\n0.6\nhi\n2.677\n1.983\n3.598\n\n\nlo\n2.166\n1.578\n2.973\n\n\n\n\n\n\n\nI won’t do the full set of pairwise contrasts here as we don’t care about most of them. As expected, there’s a clear effect of coherence upon kappa (high-coherence trials produce a less dispersed distribution of responses).\nNote also that the model has found it harder to estimate precision for the catch (60% coherence) trials. This may be due to the lower amount of data available, but looking at the posteriors for 40% coherence trials, it could also be due to identifiability issues when estimating kappa for tighter distributions (i.e., once a distribution is already ‘spiked,’ even massive increases in kappa result in almost no visible change to the response spread).\nHowever, a key question remains: within each level of coherence, is responding less precise for low-reward trials? In other words, are people making less of an effort when they aren’t going to win big?\n\n\n\n\n\n\n\n\n\n\nResponse precision (kappa) differences: high vs. low reward\n\n\n\n\n\n\n\n\n\n\n\n95% Credible Interval\n\n\n\n\nCoherence\nMedian Diff\nLower 95%\nUpper 95%\nP(hi &gt; lo)\n\n\n\n\n0.2\n0.002\n-0.062\n0.066\n0.527\n\n\n0.4\n0.123\n0.000\n0.256\n0.975\n\n\n0.6\n0.504\n0.029\n1.024\n0.981\n\n\n\n\n\n\n\nSo yes: while there’s no difference in response precision for low-coherence trials, it does appear that people are trying less hard for low-reward trials when coherence is higher.\n\n\nRandom effects\nIt’s also worth checking the random intercepts for mu (mean) from the model. While these aren’t random slopes (i.e., they don’t vary by task condition), they do give an indicator of individual propensity to demonstrate response bias on this task. Combined with the random intercepts for kappa (response precision), we can do a quick visual check: should we be concerned that the biases we’re seeing are driven by individuals whose responses are more imprecise?\n\n\n\n\n\n\n\n\n\nIn this graph, we see ‘baseline bias’: the model intercept for bias with each participant’s offset added. Overall, if anything, it looks like people who are poor responders tend to have baseline biases close to 0, which is consistent with the idea that they’re guessing more."
  },
  {
    "objectID": "responsebias_circularmodel.html#dual-colour-trials-new-analysis",
    "href": "responsebias_circularmodel.html#dual-colour-trials-new-analysis",
    "title": "Response bias modelling",
    "section": "Dual-colour trials: new analysis",
    "text": "Dual-colour trials: new analysis\nFinally, let’s analyse the dual-colour trials (those where participants saw one high-reward and one low-reward patch). These trials comprised 5% of the experiment (24/480) but only appeared in the second half of blocks, by which time participants should have been well-versed in the colour-reward contingencies for the task. Participants didn’t get score feedback for this trial type until the end of the experiment, when they learnt their cumulative score for these trials. There were an equal number of high-coherence and low-coherence dual-colour trials.\n\nIs a response bias present?\nWhen we talk about a response bias for dual-colour trials, I mean that the participant’s response was biased towards the high-reward motion direction rather the low-reward one. First let’s see how the model has estimated mu (mean) in each coherence condition.\n\n\n\n\n\n\n\n\n\n\nDual-colour trials: bias vs. 0 (degrees)\n\n\nCoherence\nMedian Bias\n95% Lower\n95% Upper\nP(&gt;0)\n\n\n\n\n0.2\n2.31\n-1.73\n6.48\n0.87\n\n\n0.4\n0.65\n-1.22\n2.60\n0.75\n\n\n\n\n\n\n\nNeither mean differs credibly from 0. It seems plausible that if we had more data, we might see a reward bias when coherence is low, but the posterior distribution is pretty wide because there are so few datapoints and performance under low coherence is inherently more noisy.\n\n\nHow does response bias change across coherence levels?\nAlthough it seems unlikely that a credible change will be detected given how wide the posterior distribution for low-coherence trials is, let’s check anyway.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n95% Credible Interval\n\n\n\n\nContrast\nMedian Diff\nLower 95%\nUpper 95%\nP(Direction)\n\n\n\n\nHigh - low coherence\n-1.63\n-6.11\n2.68\n0.76\n\n\n\n\n\n\n\nAs expected, we don’t have evidence of a credible difference in response bias across coherence levels here.\n\n\nDoes response precision change?\nKappa (response precision) for dual-colour trials is not so interesting, as the only factor being varied is coherence. As expected, response precision is much higher for high-coherence trials than low-coherence ones.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n95% Credible Interval\n\n\n\nCoherence\nMedian Kappa\n95% Lower\n95% Upper\n\n\n\n\n0.2\n0.552\n0.389\n0.757\n\n\n0.4\n1.750\n1.308\n2.290"
  }
]