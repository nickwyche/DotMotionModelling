[
  {
    "objectID": "responsebias_circularmodel.html",
    "href": "responsebias_circularmodel.html",
    "title": "Response bias modelling",
    "section": "",
    "text": "After Friday’s meeting I had a think about how we might model response bias in the dot motion task. At the moment there are a few problems with our analytic approach:\n\nIt’s a compromise. We either exclude a lot of the data (restricting range to ±90° for ANOVA) or end up with an artificial metric that’s quite difficult to interpret (Rangelov et al. regression approach)\nIt’s tough to answer questions about how responses are distributed, as opposed to simply looking at their means. As discussed, this is important if we want to assess whether people are guessing more on low-reward trials.\nIt doesn’t work very well for our minority trial types (catch trials with 60% coherence, and dual-colour trials which pit high-reward and low-reward dot clouds against each other). Data per participant is quite sparse for these trial types, so they’re less appropriate for a straightforward ANOVA technique where we take condition means per participant.\n\n\n\n\nI went back to the drawing board and realised that we can simply model circular responses directly in the R package brms. This solves all of our problems:\n\n\n\n\n\n\n\nProblem\nbrms modelling solution\n\n\nCompromise (dropping data or using an artificial metric which isn’t directly interpretable)\nUses all data while yielding a directly intepretable parameter (mu, which is essentially the distribution’s mean)\n\n\nNo info about response distribution\nCan also model effects of task condition and random effects by participant for distribution (kappa, concentration of responses)\n\n\nProblems with sparse trial-level data for rare trial types\nPartial pooling: model participants as offsets from latent population parameters. Because the model automatically downweights noisy participants and ‘shrinks’ extreme individual estimates toward the group mean, we can extract a more reliable signal from sparse trials than we could with e.g. ANOVA.\n\n\n\nThese models are based off the von Mises distribution (circular equivalent of the normal distribution), which is defined by two parameters:\n\nmu: the location parameter (where the distribution peaks)\nkappa: the concentration parameter (how tightly clustered the data are around the peak of the distribution; inverse of dispersion measures such as SD)\n\nI’ve fit two models to the data in brms, as follows:\n\nFor core and catch trials (i.e., 3 x 2 structure: 20%/40%/60% coherence x low/high reward):\n\nresponse bias (mu) ~ reward * coherence + (reward * coherence | participant)\nkappa ~ reward * coherence + (reward * coherence | participant)\n\nFor dual-colour trials (i.e., 1 x 2 structure: one high-reward and one low-reward dot cloud x 20%/40% coherence)\n\nresponse bias (mu) ~ coherence + (coherence | participant)\nkappa ~ coherence + (coherence | participant)\n\n\nNote: I realised as I went to send this off that I can actually fit the model as a 3x3 design, where the dual-colour trials are treated as another (mystery) level of reward. This will help tighten up the model fit for that trial type as reported at the end of this page. Obviously there are no mystery/60% coherence trials in our design: the model will still estimate what it thinks should happen for that trial type, but we can just ignore the output for that cell."
  },
  {
    "objectID": "responsebias_circularmodel.html#new-approach-modelling-response-bias-and-precision",
    "href": "responsebias_circularmodel.html#new-approach-modelling-response-bias-and-precision",
    "title": "Response bias modelling",
    "section": "",
    "text": "After Friday’s meeting I had a think about how we might model response bias in the dot motion task. At the moment there are a few problems with our analytic approach:\n\nIt’s a compromise. We either exclude a lot of the data (restricting range to ±90° for ANOVA) or end up with an artificial metric that’s quite difficult to interpret (Rangelov et al. regression approach)\nIt’s tough to answer questions about how responses are distributed, as opposed to simply looking at their means. As discussed, this is important if we want to assess whether people are guessing more on low-reward trials.\nIt doesn’t work very well for our minority trial types (catch trials with 60% coherence, and dual-colour trials which pit high-reward and low-reward dot clouds against each other). Data per participant is quite sparse for these trial types, so they’re less appropriate for a straightforward ANOVA technique where we take condition means per participant.\n\n\n\n\nI went back to the drawing board and realised that we can simply model circular responses directly in the R package brms. This solves all of our problems:\n\n\n\n\n\n\n\nProblem\nbrms modelling solution\n\n\nCompromise (dropping data or using an artificial metric which isn’t directly interpretable)\nUses all data while yielding a directly intepretable parameter (mu, which is essentially the distribution’s mean)\n\n\nNo info about response distribution\nCan also model effects of task condition and random effects by participant for distribution (kappa, concentration of responses)\n\n\nProblems with sparse trial-level data for rare trial types\nPartial pooling: model participants as offsets from latent population parameters. Because the model automatically downweights noisy participants and ‘shrinks’ extreme individual estimates toward the group mean, we can extract a more reliable signal from sparse trials than we could with e.g. ANOVA.\n\n\n\nThese models are based off the von Mises distribution (circular equivalent of the normal distribution), which is defined by two parameters:\n\nmu: the location parameter (where the distribution peaks)\nkappa: the concentration parameter (how tightly clustered the data are around the peak of the distribution; inverse of dispersion measures such as SD)\n\nI’ve fit two models to the data in brms, as follows:\n\nFor core and catch trials (i.e., 3 x 2 structure: 20%/40%/60% coherence x low/high reward):\n\nresponse bias (mu) ~ reward * coherence + (reward * coherence | participant)\nkappa ~ reward * coherence + (reward * coherence | participant)\n\nFor dual-colour trials (i.e., 1 x 2 structure: one high-reward and one low-reward dot cloud x 20%/40% coherence)\n\nresponse bias (mu) ~ coherence + (coherence | participant)\nkappa ~ coherence + (coherence | participant)\n\n\nNote: I realised as I went to send this off that I can actually fit the model as a 3x3 design, where the dual-colour trials are treated as another (mystery) level of reward. This will help tighten up the model fit for that trial type as reported at the end of this page. Obviously there are no mystery/60% coherence trials in our design: the model will still estimate what it thinks should happen for that trial type, but we can just ignore the output for that cell."
  },
  {
    "objectID": "responsebias_circularmodel.html#core-and-catch-trials-reanalysis",
    "href": "responsebias_circularmodel.html#core-and-catch-trials-reanalysis",
    "title": "Response bias modelling",
    "section": "2 Core and catch trials: reanalysis",
    "text": "2 Core and catch trials: reanalysis\nCore trials made up the majority (90%; 432/480 trials) of the task. They’re what we’ve discussed in meetings so far: participants saw an equal number of trials from each cell of our 2 x 2 design (2 reward levels: low, high; 2 coherence levels: 20%, 40%).\nCatch trials (those with coherence 60%) comprised 5% of the total number of trials in the experiment (24/480), with an equal number of catch trials in each block. There were an equal number of high-reward and low-reward catch trials. In theory, these trials are so easy that we shouldn’t be seeing any reward-related bias in responding.\nBecause catch trials basically add another level of coherence to the core 2 x 2 design, they can be directly added into a combined model. One thing to keep in mind is that estimation will often be less precise for 60% coherence trials as the model is informed by less data from this trial type.\n\n2.1 Does the model fit?\nFirst, let’s visually inspect how the model is fitting relative to the observed data.\n\n\n\n\n\n\n\n\n\nOverall, model fit is looking OKish. The key concern is that it’s slightly underestimating the heights of the distributions’ peaks, and for 40% coherence the ‘shoulders’ are a bit too wide. Both of these things suggest we should tweak priors for response precision, and/or fit a mixture model that accounts for guessing also. For now, this fit is near enough that I think we can take the following results as indicators of what we’d expect to see from a mixture modelling approach.\n\n\n2.2 Is a response bias present?\nFirst let’s see how the model has estimated mu (mean) in each reward x coherence condition.\n\n\n\nResponse bias by condition (degrees)\n\n\nReward\nCoherence\nMedian Bias\n95% Lower\n95% Upper\nP(&gt;0)\n\n\n\n\nhi\n0.2\n4.36\n2.16\n6.51\n1.00\n\n\nhi\n0.4\n2.06\n0.26\n3.83\n0.99\n\n\nhi\n0.6\n1.61\n-0.52\n3.62\n0.93\n\n\nlo\n0.2\n1.50\n-0.65\n3.64\n0.91\n\n\nlo\n0.4\n2.64\n0.91\n4.37\n1.00\n\n\nlo\n0.6\n3.03\n0.74\n5.29\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTakeaways:\n\nWhen coherence is low (20%), there is a credible response bias for high-value trials but not low-value ones\nWhen coherence is medium (40%), there is a credible (but small) response bias for both high and low-value trials.\nWhen coherence is high (60%), there is no longer a response bias for high-value trials. Conversely, the response bias for low-value trials persists.\n\n\n\n2.3 How does bias change with coherence and reward?\nThe nice thing about this modelling approach is that we can see the posterior distributions for differences in response bias across levels of coherence and reward (i.e., the range of plausible values for how much the response has actually shifted). In these graphs, the plotted point is the median difference and the whiskers are the 66% (thick) and 95% (thin) credible intervals.\n\n2.3.1 Effect of coherence\nFirstly, let’s take a look at how coherence affects response bias within each level of reward.\n\n\n\n\n\n\n\n\n\n\nChange in bias across coherence levels\n\n\nReward Level Contrast\nMedian Diff\nLower 95%\nUpper 95%\nP(Diff)\n\n\n\n\nhi coherence0.2 - hi coherence0.4\n2.30\n0.68\n3.89\n1.00\n\n\nlo coherence0.2 - lo coherence0.4\n-1.14\n-2.81\n0.51\n0.91\n\n\nhi coherence0.4 - hi coherence0.6\n0.46\n-1.06\n1.99\n0.73\n\n\nlo coherence0.4 - lo coherence0.6\n-0.40\n-2.15\n1.40\n0.67\n\n\nhi coherence0.2 - hi coherence0.6\n2.76\n0.81\n4.70\n1.00\n\n\nlo coherence0.2 - lo coherence0.6\n-1.53\n-3.67\n0.63\n0.92\n\n\n\n\n\n\n\nTakeaways:\n\nFor low-reward trials, the bias is stable: it doesn’t reliably change across any of the coherence comparisons\nFor high-reward trials, the exceptional condition is low coherence: bias in this condition is credibly higher than in either the medium or high coherence condition. Note that the bias doesn’t credibly differ across the two higher-coherence conditions.\n\n\n\n2.3.2 Effect of reward\n\n\n\n\n\n\n\n\n\n\nCore trials: change in bias across reward levels\n\n\nCoherence Level\nMedian Diff\nLower 95%\nUpper 95%\nP(Diff)\n\n\n\n\nhi coherence0.2 - lo coherence0.2\n2.86\n0.78\n4.95\n1.00\n\n\nhi coherence0.4 - lo coherence0.4\n-0.57\n-1.70\n0.53\n0.85\n\n\nhi coherence0.6 - lo coherence0.6\n-1.43\n-3.46\n0.61\n0.92\n\n\n\n\n\n\n\nTakeaways:\n\nWhen coherence is low, response bias is credibly larger for high-reward trials than low-reward ones\nWhen coherence is medium and high, there is no credible difference in response bias based on reward level, but the trend suggests the bias may be inverting (i.e., crossover interaction: as coherence increases, participants show more bias on low-value trials than high-value ones)\n\n\n\n\n2.4 Does response precision change across trial types?\nNext, let’s see how the model has estimated kappa (response precision) in each reward x coherence condition.\n\n\n\n\n\n\n\n\n\n\nCore trials: response precision (kappa) by condition\n\n\n\n\n\n\n\n\n\n\n\n95% Credible Interval\n\n\n\nCoherence\nReward\nMedian Kappa\n95% Lower\n95% Upper\n\n\n\n\n0.2\nlo\n0.597\n0.486\n0.737\n\n\nhi\n0.599\n0.483\n0.742\n\n\n0.4\nlo\n1.370\n1.115\n1.684\n\n\nhi\n1.491\n1.204\n1.853\n\n\n0.6\nlo\n2.166\n1.578\n2.973\n\n\nhi\n2.677\n1.983\n3.598\n\n\n\n\n\n\n\nI won’t do the full set of pairwise contrasts here as we don’t care about most of them. As expected, there’s a clear effect of coherence upon kappa (high-coherence trials produce a less dispersed distribution of responses).\nNote also that the model has found it harder to estimate precision for the catch (60% coherence) trials. This may be due to the lower amount of data available, but looking at the posteriors for 40% coherence trials, it could also be due to identifiability issues when estimating kappa for tighter distributions (i.e., once a distribution is already ‘spiked,’ even massive increases in kappa result in almost no visible change to the response spread).\nHowever, a key question remains: within each level of coherence, is responding less precise for low-reward trials? In other words, are people making less of an effort when they aren’t going to win big?\n\n\n\n\n\n\n\n\n\n\nResponse precision (kappa) differences: high vs. low reward\n\n\n\n\n\n\n\n\n\n\n\n95% Credible Interval\n\n\n\n\nCoherence\nMedian Diff\nLower 95%\nUpper 95%\nP(hi &gt; lo)\n\n\n\n\n0.2\n0.002\n-0.062\n0.066\n0.527\n\n\n0.4\n0.123\n0.000\n0.256\n0.975\n\n\n0.6\n0.504\n0.029\n1.024\n0.981\n\n\n\n\n\n\n\nSo yes: while there’s no difference in response precision for low-coherence trials, it does appear that people are trying less hard for low-reward trials when coherence is higher.\n\n\n2.5 Random effects\nIt’s also worth checking the model’s random effects, which give an indicator of individual propensity to demonstrate response bias in each task condition. Let’s do a quick visual check: should we be concerned that the biases we’re seeing are driven by individuals whose responses are more imprecise?\n\n\n\n\n\n\n\n\n\nIn this graph, we see the condition intercept for bias with each participant’s condition offset added. Overall, if anything, it looks like people who are poor responders tend to have baseline biases close to 0, which is consistent with the idea that they’re guessing more and their responses are randomly distributed."
  },
  {
    "objectID": "responsebias_circularmodel.html#dual-colour-trials-new-analysis",
    "href": "responsebias_circularmodel.html#dual-colour-trials-new-analysis",
    "title": "Response bias modelling",
    "section": "3 Dual-colour trials: new analysis",
    "text": "3 Dual-colour trials: new analysis\nFinally, let’s analyse the dual-colour trials (those where participants saw one high-reward and one low-reward patch). These trials comprised 5% of the experiment (24/480) but only appeared in the second half of blocks, by which time participants should have been well-versed in the colour-reward contingencies for the task. Participants didn’t get score feedback for this trial type until the end of the experiment, when they learnt their cumulative score for these trials. There were an equal number of high-coherence and low-coherence dual-colour trials.\n\n3.1 Is a response bias present?\nWhen we talk about a response bias for dual-colour trials, I mean that the participant’s response was biased towards the high-reward motion direction rather the low-reward one. First let’s see how the model has estimated mu (mean) in each coherence condition.\n\n\n\n\n\n\n\n\n\n\nDual-colour trials: bias vs. 0 (degrees)\n\n\nCoherence\nMedian Bias\n95% Lower\n95% Upper\nP(&gt;0)\n\n\n\n\n0.2\n2.31\n-1.73\n6.48\n0.87\n\n\n0.4\n0.65\n-1.22\n2.60\n0.75\n\n\n\n\n\n\n\nNeither mean differs credibly from 0. It seems plausible that if we had more data, we might see a reward bias when coherence is low, but the posterior distribution is pretty wide because there are so few datapoints and performance under low coherence is inherently more noisy.\n\n\n3.2 How does response bias change across coherence levels?\nAlthough it seems unlikely that a credible change will be detected given how wide the posterior distribution for low-coherence trials is, let’s check anyway.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n95% Credible Interval\n\n\n\n\nContrast\nMedian Diff\nLower 95%\nUpper 95%\nP(Direction)\n\n\n\n\nHigh - low coherence\n-1.63\n-6.11\n2.68\n0.76\n\n\n\n\n\n\n\nAs expected, we don’t have evidence of a credible difference in response bias across coherence levels here.\n\n\n3.3 Does response precision change?\nKappa (response precision) for dual-colour trials is not so interesting, as the only factor being varied is coherence. As expected, response precision is much higher for high-coherence trials than low-coherence ones.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n95% Credible Interval\n\n\n\nCoherence\nMedian Kappa\n95% Lower\n95% Upper\n\n\n\n\n0.2\n0.552\n0.389\n0.757\n\n\n0.4\n1.750\n1.308\n2.290"
  }
]